{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load setup_enviroment.py\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_02.csv', 'valid_01.csv', 'test_07.csv', 'valid_03.csv', 'test_03.csv', 'train_12.csv', 'test_04.csv', 'train_04.csv', 'train_00.csv', 'valid_06.csv', 'valid_05.csv', 'train_13.csv', 'test_02.csv', 'train_01.csv', 'train_14.csv', 'train_05.csv', 'valid_00.csv', 'test_09.csv', 'valid_09.csv', 'train_08.csv', 'train_16.csv', 'train_10.csv', 'valid_02.csv', 'train_07.csv', 'train_11.csv', 'train_15.csv', 'valid_07.csv', 'train_19.csv', 'test_08.csv', 'train_06.csv', 'train_18.csv', 'test_01.csv', 'train_03.csv', 'train_17.csv', 'test_05.csv', 'valid_04.csv', 'test_00.csv', 'valid_08.csv', 'train_09.csv', 'test_06.csv']\n"
     ]
    }
   ],
   "source": [
    "source_dir = \"./generate_csv/\"\n",
    "\n",
    "print(os.listdir(source_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./generate_csv/train_02.csv',\n",
      " './generate_csv/train_12.csv',\n",
      " './generate_csv/train_04.csv',\n",
      " './generate_csv/train_00.csv',\n",
      " './generate_csv/train_13.csv',\n",
      " './generate_csv/train_01.csv',\n",
      " './generate_csv/train_14.csv',\n",
      " './generate_csv/train_05.csv',\n",
      " './generate_csv/train_08.csv',\n",
      " './generate_csv/train_16.csv',\n",
      " './generate_csv/train_10.csv',\n",
      " './generate_csv/train_07.csv',\n",
      " './generate_csv/train_11.csv',\n",
      " './generate_csv/train_15.csv',\n",
      " './generate_csv/train_19.csv',\n",
      " './generate_csv/train_06.csv',\n",
      " './generate_csv/train_18.csv',\n",
      " './generate_csv/train_03.csv',\n",
      " './generate_csv/train_17.csv',\n",
      " './generate_csv/train_09.csv']\n",
      "['./generate_csv/valid_01.csv',\n",
      " './generate_csv/valid_03.csv',\n",
      " './generate_csv/valid_06.csv',\n",
      " './generate_csv/valid_05.csv',\n",
      " './generate_csv/valid_00.csv',\n",
      " './generate_csv/valid_09.csv',\n",
      " './generate_csv/valid_02.csv',\n",
      " './generate_csv/valid_07.csv',\n",
      " './generate_csv/valid_04.csv',\n",
      " './generate_csv/valid_08.csv']\n",
      "['./generate_csv/test_07.csv',\n",
      " './generate_csv/test_03.csv',\n",
      " './generate_csv/test_04.csv',\n",
      " './generate_csv/test_02.csv',\n",
      " './generate_csv/test_09.csv',\n",
      " './generate_csv/test_08.csv',\n",
      " './generate_csv/test_01.csv',\n",
      " './generate_csv/test_05.csv',\n",
      " './generate_csv/test_00.csv',\n",
      " './generate_csv/test_06.csv']\n"
     ]
    }
   ],
   "source": [
    "def get_filename_by_prefix(source_dir,prefix_name):\n",
    "    all_files = os.listdir(source_dir)\n",
    "    result = []\n",
    "    for filename in all_files:\n",
    "        if filename.startswith(prefix_name):\n",
    "            result.append(os.path.join(source_dir,filename))\n",
    "    return result\n",
    "\n",
    "train_filenames = get_filename_by_prefix(source_dir,\"train\")\n",
    "valid_filenames = get_filename_by_prefix(source_dir,\"valid\")\n",
    "test_filenames = get_filename_by_prefix(source_dir,\"test\")\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(train_filenames)\n",
    "pprint.pprint(valid_filenames)\n",
    "pprint.pprint(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_line(line,n_fields =9):\n",
    "    defs = [tf.constant(np.nan)] * n_fields\n",
    "    parsed_fields = tf.io.decode_csv(line,record_defaults=defs)\n",
    "    x = tf.stack(parsed_fields[0:-1])\n",
    "    y = tf.stack(parsed_fields[-1:])\n",
    "    return x,y\n",
    "\n",
    "def csv_read_dataset(filenames,n_reads = 5,\n",
    "                     batch_size = 32,n_parse_threads = 5,\n",
    "                     shuffle_buffer_size = 10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename:tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length = n_reads)\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_csv_line,\n",
    "                          num_parallel_calls = n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_set = csv_read_dataset(train_filenames,batch_size = batch_size)\n",
    "valid_set = csv_read_dataset(valid_filenames,batch_size = batch_size)\n",
    "test_set = csv_read_dataset(test_filenames,batch_size = batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(x,y):\n",
    "    \"\"\"convert x,y to tf.train.Example and serialize\"\"\"\n",
    "    input_features = tf.train.FloatList(value = x)\n",
    "    label = tf.train.FloatList(value = y)\n",
    "    \n",
    "    features = tf.train.Features(\n",
    "        feature = {\n",
    "            \"input_features\" : tf.train.Feature(\n",
    "                float_list = input_features),\n",
    "            \"label\" : tf.train.Feature(\n",
    "                float_list = label)\n",
    "        }\n",
    "    )\n",
    "    example = tf.train.Example(features = features)\n",
    "    return example.SerializeToString()\n",
    "\n",
    "def csv_dataset_to_tfrecords(base_filenames,dataset,\n",
    "                             n_shards,step_per_shard,\n",
    "                             compression_type = None):\n",
    "    all_filenames = []\n",
    "    \n",
    "    options = tf.io.TFRecordOptions(compression_type = compression_type)\n",
    "    for shard_id in range(n_shards):\n",
    "        filename_fullpath = \"{}_{:05d}-of-{:05d}\".format(base_filenames,\n",
    "                                                         shard_id,\n",
    "                                                         n_shards)\n",
    "        with tf.io.TFRecordWriter(filename_fullpath,options) as writer:\n",
    "            for x_batch,y_batch in dataset.take(step_per_shard):\n",
    "                for x_example,y_example in zip(x_batch,y_batch):\n",
    "                    writer.write(\n",
    "                        serialize_example(x_example,y_example))\n",
    "        all_filenames.append(filename_fullpath)\n",
    "        \n",
    "    return all_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shards = 20\n",
    "\n",
    "train_step_per_shard = 11610 // batch_size // n_shards\n",
    "\n",
    "valid_step_per_shard = 3380 // batch_size // n_shards\n",
    "\n",
    "test_step_per_shard = 5170 // batch_size // n_shards\n",
    "\n",
    "output_dir = \"generate_tfrecords\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "train_basename = os.path.join(output_dir,\"train\")\n",
    "valid_basename = os.path.join(output_dir,\"valid\")\n",
    "test_basename = os.path.join(output_dir,\"test\")\n",
    "\n",
    "train_tfrecord_filenames = csv_dataset_to_tfrecords(train_basename,\n",
    "                                                    train_set,\n",
    "                                                    n_shards,\n",
    "                                                    train_step_per_shard,\n",
    "                                                    None)\n",
    "valid_tfrecord_filenames = csv_dataset_to_tfrecords(valid_basename,\n",
    "                                                    valid_set,\n",
    "                                                    n_shards,\n",
    "                                                    valid_step_per_shard,\n",
    "                                                    None)\n",
    "test_tfrecord_filenames = csv_dataset_to_tfrecords(test_basename,\n",
    "                                                   test_set,\n",
    "                                                   n_shards,\n",
    "                                                   test_step_per_shard,\n",
    "                                                   None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shards = 20\n",
    "\n",
    "train_step_per_shard = 11610 // batch_size // n_shards\n",
    "\n",
    "valid_step_per_shard = 3380 // batch_size // n_shards\n",
    "\n",
    "test_step_per_shard = 5170 // batch_size // n_shards\n",
    "\n",
    "output_dir = \"generate_tfrecords_zip\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "train_basename = os.path.join(output_dir,\"train\")\n",
    "valid_basename = os.path.join(output_dir,\"valid\")\n",
    "test_basename = os.path.join(output_dir,\"test\")\n",
    "\n",
    "train_tfrecord_filenames = csv_dataset_to_tfrecords(train_basename,\n",
    "                                                    train_set,\n",
    "                                                    n_shards,\n",
    "                                                    train_step_per_shard,\n",
    "                                                    compression_type= \"GZIP\")\n",
    "valid_tfrecord_filenames = csv_dataset_to_tfrecords(valid_basename,\n",
    "                                                    valid_set,\n",
    "                                                    n_shards,\n",
    "                                                    valid_step_per_shard,\n",
    "                                                    compression_type= \"GZIP\")\n",
    "test_tfrecord_filenames = csv_dataset_to_tfrecords(test_basename,\n",
    "                                                   test_set,\n",
    "                                                   n_shards,\n",
    "                                                   test_step_per_shard,\n",
    "                                                   compression_type= \"GZIP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_tfrecords_zip/train_00000-of-00020',\n",
      " 'generate_tfrecords_zip/train_00001-of-00020',\n",
      " 'generate_tfrecords_zip/train_00002-of-00020',\n",
      " 'generate_tfrecords_zip/train_00003-of-00020',\n",
      " 'generate_tfrecords_zip/train_00004-of-00020',\n",
      " 'generate_tfrecords_zip/train_00005-of-00020',\n",
      " 'generate_tfrecords_zip/train_00006-of-00020',\n",
      " 'generate_tfrecords_zip/train_00007-of-00020',\n",
      " 'generate_tfrecords_zip/train_00008-of-00020',\n",
      " 'generate_tfrecords_zip/train_00009-of-00020',\n",
      " 'generate_tfrecords_zip/train_00010-of-00020',\n",
      " 'generate_tfrecords_zip/train_00011-of-00020',\n",
      " 'generate_tfrecords_zip/train_00012-of-00020',\n",
      " 'generate_tfrecords_zip/train_00013-of-00020',\n",
      " 'generate_tfrecords_zip/train_00014-of-00020',\n",
      " 'generate_tfrecords_zip/train_00015-of-00020',\n",
      " 'generate_tfrecords_zip/train_00016-of-00020',\n",
      " 'generate_tfrecords_zip/train_00017-of-00020',\n",
      " 'generate_tfrecords_zip/train_00018-of-00020',\n",
      " 'generate_tfrecords_zip/train_00019-of-00020']\n",
      "['generate_tfrecords_zip/valid_00000-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00001-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00002-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00003-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00004-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00005-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00006-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00007-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00008-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00009-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00010-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00011-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00012-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00013-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00014-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00015-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00016-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00017-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00018-of-00020',\n",
      " 'generate_tfrecords_zip/valid_00019-of-00020']\n",
      "['generate_tfrecords_zip/test_00000-of-00020',\n",
      " 'generate_tfrecords_zip/test_00001-of-00020',\n",
      " 'generate_tfrecords_zip/test_00002-of-00020',\n",
      " 'generate_tfrecords_zip/test_00003-of-00020',\n",
      " 'generate_tfrecords_zip/test_00004-of-00020',\n",
      " 'generate_tfrecords_zip/test_00005-of-00020',\n",
      " 'generate_tfrecords_zip/test_00006-of-00020',\n",
      " 'generate_tfrecords_zip/test_00007-of-00020',\n",
      " 'generate_tfrecords_zip/test_00008-of-00020',\n",
      " 'generate_tfrecords_zip/test_00009-of-00020',\n",
      " 'generate_tfrecords_zip/test_00010-of-00020',\n",
      " 'generate_tfrecords_zip/test_00011-of-00020',\n",
      " 'generate_tfrecords_zip/test_00012-of-00020',\n",
      " 'generate_tfrecords_zip/test_00013-of-00020',\n",
      " 'generate_tfrecords_zip/test_00014-of-00020',\n",
      " 'generate_tfrecords_zip/test_00015-of-00020',\n",
      " 'generate_tfrecords_zip/test_00016-of-00020',\n",
      " 'generate_tfrecords_zip/test_00017-of-00020',\n",
      " 'generate_tfrecords_zip/test_00018-of-00020',\n",
      " 'generate_tfrecords_zip/test_00019-of-00020']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(train_tfrecord_filenames)\n",
    "pprint.pprint(valid_tfrecord_filenames)\n",
    "pprint.pprint(test_tfrecord_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.118108   -0.84895366 -0.84369123 -0.02051749  0.19971783 -0.00389153\n",
      "  -1.3487345   1.2535764 ]\n",
      " [ 0.84634066 -0.68987757 -0.3091761  -0.42383355 -1.0479033  -0.1685368\n",
      "   1.077483   -1.2344942 ]\n",
      " [ 0.42359042  0.58273077  0.055551   -0.17149982  0.4782607  -0.10523725\n",
      "   0.965288   -1.2645314 ]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.25 ]\n",
      " [1.908]\n",
      " [2.068]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.3701508   1.1394969   0.20343983 -0.09859075 -0.6797969   0.05049406\n",
      "  -0.7129627   0.7880018 ]\n",
      " [-0.22520569 -0.05357341 -0.2128554  -0.09616713  0.14239712  0.44052222\n",
      "  -0.88593006  0.84306973]\n",
      " [-0.44235379  0.82134485 -0.5079775  -0.08779322  0.04566842  0.19360971\n",
      "  -0.7176375   0.66785353]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.148]\n",
      " [1.595]\n",
      " [1.563]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "expect_features = {\n",
    "    \"input_features\":tf.io.FixedLenFeature([8],dtype = tf.float32),\n",
    "    \"label\" : tf.io.FixedLenFeature([1],dtype = tf.float32)\n",
    "}\n",
    "\n",
    "def parse_example(serialized_example):\n",
    "    example = tf.io.parse_single_example(serialized_example,\n",
    "                                         expect_features)\n",
    "    return example[\"input_features\"],example[\"label\"]\n",
    "\n",
    "def tfrecords_read_dataset(filenames,n_reads = 5,\n",
    "                     batch_size = 32,n_parse_threads = 5,\n",
    "                     shuffle_buffer_size = 10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename:tf.data.TFRecordDataset(filename,compression_type = \"GZIP\"),\n",
    "        cycle_length = n_reads)\n",
    "    dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(parse_example,\n",
    "                          num_parallel_calls = n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "tfrecords_train = tfrecords_read_dataset(train_tfrecord_filenames,batch_size = 3)\n",
    "for x_batch,y_batch in tfrecords_train.take(2):\n",
    "    print(x_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "tfrecord_train_set = tfrecords_read_dataset(train_tfrecord_filenames,\n",
    "                                            batch_size = batch_size)\n",
    "tfrecord_valid_set = tfrecords_read_dataset(valid_tfrecord_filenames,\n",
    "                                            batch_size = batch_size)\n",
    "tfrecord_test_set = tfrecords_read_dataset(test_tfrecord_filenames,\n",
    "                                           batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 348 steps, validate for 118 steps\n",
      "Epoch 1/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.7110 - val_loss: 0.4961\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4599 - val_loss: 0.4520\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4334 - val_loss: 0.7368\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.4027 - val_loss: 1.0570\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3890 - val_loss: 1.1418\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3853 - val_loss: 1.4853\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3633 - val_loss: 1.4113\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30,activation=\"relu\",input_shape=[8]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss = \"mean_squared_error\",\n",
    "              optimizer = \"sgd\",)\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience = 5,\n",
    "                                           min_delta = 1e-2)]\n",
    "\n",
    "history = model.fit(tfrecord_train_set,\n",
    "                    validation_data = tfrecord_valid_set,\n",
    "                    steps_per_epoch = 11160 // batch_size,\n",
    "                    validation_steps = 3780 // batch_size,\n",
    "                    epochs = 100,\n",
    "                    callbacks = callbacks,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 2ms/step - loss: 1.1283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1282846205260442"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tfrecord_test_set,steps = 5160 // batch_size,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
