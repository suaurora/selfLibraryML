{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*********************************************************\n",
    "#Same thing as it going to do with embedding rnn , and I \n",
    "#won't run it again\n",
    "#*********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"shakespeare.txt\").read()\n",
    "\n",
    "print(len(text))\n",
    "print(text[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "\n",
    "print(len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {char:idx for idx,char in enumerate(vocab)}\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = np.array(vocab)\n",
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "print(text_as_int[0:10])\n",
    "print(text[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(id_text):\n",
    "    return id_text[0:-1],id_text[1:]\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "seq_length = 100\n",
    "seq_dataset = char_dataset.batch(seq_length+1,\n",
    "                                 drop_remainder = True)\n",
    "\n",
    "for ch_id in char_dataset.take(2):\n",
    "    print(ch_id,idx2char[ch_id.numpy()])\n",
    "    \n",
    "for seq_id in seq_dataset.take(2):\n",
    "    print(seq_id)\n",
    "    print(repr(\"\".join(idx2char[seq_id.numpy()])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_dataset = seq_dataset.map(split_input_target)\n",
    "\n",
    "for item_input,item_output in seq_dataset.take(2):\n",
    "    print(item_input.numpy())\n",
    "    print(item_output.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "\n",
    "seq_dataset = seq_dataset.shuffle(buffer_size).batch(batch_size,\n",
    "                                                     drop_remainder = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size,embedding_dim,rnn_units,batch_size):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(vocab_size,embedding_dim,\n",
    "                               batch_input_shape = [batch_size,None]),\n",
    "        keras.layers.LSTM(units = rnn_units,\n",
    "                          return_sequences = True,\n",
    "                          #stateful = True,\n",
    "                          #recurrent_initialize = \"glorot_uniform\",\n",
    "                          ),\n",
    "        #return_sequences: Boolean.\n",
    "        #Whether to return the last output in the output sequence\n",
    "        #or the full sequence\n",
    "        keras.layers.Dense(vocab_size)\n",
    "        #not a activation?\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size = vocab_size,\n",
    "                    embedding_dim = embedding_dim,\n",
    "                    rnn_units = rnn_units,\n",
    "                    batch_size = batch_size)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example_batch,target_example_batch in seq_dataset.take(10):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random sampling\n",
    "\n",
    "sample_indices = tf.random.categorical(logits = example_batch_predictions[0],\n",
    "                                       num_samples = 1)\n",
    "#the numerate before we do softmax\n",
    "#the finall layer we do not add a activation so\n",
    "#here we use logits\n",
    "print(sample_indices)\n",
    "print(example_batch_predictions[0])\n",
    "#sampling from 65 category to 100 positions \n",
    "sample_indices = tf.squeeze(sample_indices,axis = -1)\n",
    "print(sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input: \",repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print(\"Output: \",repr(\"\".join(idx2char[target_example_batch[0]])))\n",
    "print(\"Preditions: \",repr(\"\".join(idx2char[sample_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels,logits):\n",
    "    return keras.losses.sparse_categorical_crossentropy(labels,\n",
    "                                                        logits,\n",
    "                                                        from_logits = True)\n",
    "\n",
    "model.compile(optimizer = \"adam\",\n",
    "              loss = loss)\n",
    "\n",
    "example_loss = loss(target_example_batch,example_batch_predictions)\n",
    "print(example_loss.shape)\n",
    "print(example_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./text_generation_checkpoints\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "checkpoint_prefix = os.path.join(output_dir,\"ckpt_{epoch}\")\n",
    "checkpoint_callbacks = keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    save_weights_only = True)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "history = model.fit(seq_dataset,\n",
    "                    epochs = epochs,\n",
    "                    callbacks = [checkpoint_callbacks],\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = build_model(vocab_size = vocab_size,\n",
    "                     embedding_dim = embedding_dim,\n",
    "                     batch_size = 1,\n",
    "                     rnn_units = rnn_units)\n",
    "\n",
    "model2.load_weights(tf.train.latest_checkpoint(output_dir))\n",
    "model2.build(tf.TensorShape([1,None]))\n",
    "#1 for batch_size\n",
    "#start character sequence A\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,start_string,num_generate = 1000):\n",
    "    input_eval = [char2idx[c] for c in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval,0)\n",
    "    \n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    #************************************************************\n",
    "    #temperture = 0.5\n",
    "    ##when you set a temperture as you can see softmax is activarion\n",
    "    ##when doing a softmax to the result\n",
    "    ##eg [4,2] -> softmax [e^4 / (e^4 + e^2),e^2 / (e^4 + e^2)]\n",
    "    ##eg [2,1] -> softmax [e^2 / (e^2 + e^1),e^1 / (e^2 + e^1)]\n",
    "    ##so when temperture < 1,we have highlight the bigger value\n",
    "    ##and when temperture > 1,we make the result more flatten \n",
    "    #************************************************************\n",
    "    \n",
    "    for _ in range(num_generate):\n",
    "        #1. model inference -> prediction\n",
    "        #2. sample -> ch -> text_generated\n",
    "        #why sample?\n",
    "        #3. update input_eval\n",
    "        predictions = model(input_eval)\n",
    "        #predictions : [batch_size * input_eval_length * vocab_size]\n",
    "        #************************************************************\n",
    "        #predictions = predictions / temperture\n",
    "        #************************************************************\n",
    "        predictions = tf.squeeze(predictions,0)\n",
    "        #predictions : [input_eval_length * vocab_size]\n",
    "        #in this case batch_size == 1 \n",
    "        predicted_ids = tf.random.categorical(predictions,num_samples = 1)[-1,0].numpy()\n",
    "        #when tf.random.categorical run it requires a 2 dims predictions\n",
    "        #predicted_ids: [input_eval,1]\n",
    "        #it mean sample form the vocab_size scale with a guide of\n",
    "        #the weight of the model2\n",
    "        text_generated.append(idx2char[predicted_ids])\n",
    "        input_eval = tf.expand_dims([predicted_ids],0)\n",
    "        \n",
    "    return start_string + \"\".join(text_generated)\n",
    "\n",
    "new_text = generate_text(model2,\"All: \")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tf.random.categorical(tf.math.log([[0.5,0.5]]), 5)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
